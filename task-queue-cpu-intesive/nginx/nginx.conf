# ============================================================================
# NGINX PRODUCTION CONFIGURATION
# ============================================================================
# Architecture: Layer 7 Load Balancer with Health Checks
# Patterns: Circuit breaker, connection pooling, request tracing
# Performance: Keep-alive connections, gzip compression, rate limiting
# ============================================================================

# Worker processes = auto (matches CPU cores for optimal performance)
# Why auto? Nginx workers are single-threaded, 1 per core maximizes CPU usage
worker_processes auto;

# PID file location
pid /var/run/nginx.pid;

# Error log configuration
# Why notice level? Balance between verbosity and disk usage
error_log /var/log/nginx/error.log notice;

# Events block: Connection handling optimization
events {
    # Max connections per worker (default 1024)
    # Total capacity = worker_processes × worker_connections
    # Example: 4 cores × 1024 = 4096 concurrent connections
    worker_connections 1024;
    
    # Use epoll for Linux (most efficient event model)
    use epoll;
    
    # Accept multiple connections at once (performance optimization)
    multi_accept on;
}

# HTTP block: Main configuration
http {
    # MIME types
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # ========================================================================
    # LOGGING: Custom format with performance metrics
    # ========================================================================
    # Why custom format? Includes request ID, timing, upstream info
    log_format main '$remote_addr - $remote_user [$time_local] '
                    '"$request" $status $body_bytes_sent '
                    '"$http_referer" "$http_user_agent" '
                    'rt=$request_time uct="$upstream_connect_time" '
                    'uht="$upstream_header_time" urt="$upstream_response_time" '
                    'correlation_id=$correlation_id';

    access_log /var/log/nginx/access.log main;

    # ========================================================================
    # PERFORMANCE OPTIMIZATION
    # ========================================================================
    
    # Sendfile: Efficient file transfer (kernel space, bypasses user space)
    sendfile on;
    
    # TCP optimizations
    tcp_nopush on;      # Send headers in one packet with sendfile
    tcp_nodelay on;     # Disable Nagle's algorithm for low latency
    
    # Keep-alive connections (reuse TCP connections)
    # Why? Reduces handshake overhead (3-way TCP + TLS = 100-200ms saved)
    keepalive_timeout 65;
    keepalive_requests 100;
    
    # Response compression (reduces bandwidth by ~70%)
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types text/plain text/css text/xml text/javascript 
               application/json application/javascript application/xml+rss 
               application/rss+xml font/truetype font/opentype 
               application/vnd.ms-fontobject image/svg+xml;

    # Client request limits (prevent abuse)
    client_max_body_size 10M;        # Max upload size
    client_body_timeout 12s;         # Timeout for reading client body
    client_header_timeout 12s;       # Timeout for reading client headers
    send_timeout 10s;                # Timeout for sending response

    # Buffer sizes (tuned for API responses)
    client_body_buffer_size 128k;
    client_header_buffer_size 1k;
    large_client_header_buffers 4 4k;

    # ========================================================================
    # RATE LIMITING: Prevent DDoS and abuse
    # ========================================================================
    # Pattern: Token bucket algorithm
    # 10r/s = 10 requests per second per IP
    # 10m = 10MB zone (stores ~160,000 IP addresses)
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;
    
    # Limit concurrent connections per IP
    limit_conn_zone $binary_remote_addr zone=conn_limit:10m;

    # ========================================================================
    # CORRELATION ID: Use client's X-Request-ID or generate one
    # ========================================================================
    # Map directive: Set $correlation_id based on presence of X-Request-ID header
    # If client sends X-Request-ID → use it (end-to-end tracing)
    # If no X-Request-ID → use NGINX's $request_id (NGINX-generated UUID)
    map $http_x_request_id $correlation_id {
        default $http_x_request_id;
        ""      $request_id;
    }

    # ========================================================================
    # UPSTREAM: FastAPI Backend Pool
    # ========================================================================
    # Load balancing algorithm: least_conn
    # Why least_conn? Better for long-running requests (5-6s tasks)
    # Alternative: round-robin (simpler but unbalanced for async work)
    upstream fastapi_backend {
        # least_conn: Route to server with fewest active connections
        least_conn;
        
        # Circuit breaker parameters:
        # - max_fails=3: Mark server down after 3 failures
        # - fail_timeout=30s: Wait 30s before retrying failed server
        # - backup: Only use if all primary servers are down
        
        server fastapi-1:8000 max_fails=3 fail_timeout=30s;
        server fastapi-2:8000 max_fails=3 fail_timeout=30s;
        server fastapi-3:8000 max_fails=3 fail_timeout=30s;
        
        # Connection pooling: Reuse connections to upstreams
        # Why 32? Balance between resource usage and performance
        # Each idle connection = ~4KB memory, 32 × 4KB = 128KB per worker
        keepalive 32;
        keepalive_requests 100;
        keepalive_timeout 60s;
    }

    # ========================================================================
    # SERVER BLOCK: HTTP Server Configuration
    # ========================================================================
    server {
        listen 80;
        listen [::]:80;
        
        server_name localhost;
        
        # Request ID tracing (preserve client's X-Request-ID if sent)
        # FIXED: Use client's X-Request-ID or fallback to NGINX-generated $request_id
        # Why? Enable end-to-end distributed tracing from client → NGINX → FastAPI → Celery
        add_header X-Request-ID $correlation_id always;

        # Security headers (basic hardening)
        add_header X-Content-Type-Options "nosniff" always;
        add_header X-Frame-Options "SAMEORIGIN" always;
        add_header X-XSS-Protection "1; mode=block" always;

        # ====================================================================
        # HEALTH CHECK ENDPOINT (Nginx itself)
        # ====================================================================
        # Used by Docker health checks and monitoring systems
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }

        # ====================================================================
        # NGINX STATUS (for monitoring)
        # ====================================================================
        # Exposes basic Nginx metrics (active connections, requests, etc.)
        location /nginx_status {
            stub_status on;
            access_log off;
            # Restrict to localhost only in production
            allow 127.0.0.1;
            deny all;
        }

        # ====================================================================
        # API ENDPOINTS: Proxy to FastAPI backend
        # ====================================================================
        location / {
            # Apply rate limiting
            # burst=20: Allow 20 requests above rate limit (buffer for spikes)
            # nodelay: Don't delay requests within burst limit
            limit_req zone=api_limit burst=20 nodelay;
            limit_conn conn_limit 10;

            # Proxy headers (preserve client information)
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            # FIXED: Use $correlation_id (client's X-Request-ID or NGINX-generated)
            proxy_set_header X-Request-ID $correlation_id;

            # Proxy to upstream pool
            proxy_pass http://fastapi_backend;

            # HTTP version (required for keepalive)
            proxy_http_version 1.1;

            # Connection pooling (reuse upstream connections)
            proxy_set_header Connection "";

            # Timeouts (tuned for 5-6s task submission)
            proxy_connect_timeout 5s;
            proxy_send_timeout 10s;
            proxy_read_timeout 30s;

            # Buffering (trade-off: memory vs latency)
            # Why disabled? Lower latency for streaming responses
            proxy_buffering off;
            proxy_request_buffering on;

            # Retry logic (circuit breaker pattern)
            # next_upstream: Retry on these error conditions
            proxy_next_upstream error timeout http_502 http_503 http_504;
            proxy_next_upstream_tries 2;
            proxy_next_upstream_timeout 10s;
        }

        # ====================================================================
        # API DOCUMENTATION (FastAPI auto-generated docs)
        # ====================================================================
        location /docs {
            proxy_pass http://fastapi_backend/docs;
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            proxy_set_header Connection "";
        }

        location /redoc {
            proxy_pass http://fastapi_backend/redoc;
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            proxy_set_header Connection "";
        }

        location /openapi.json {
            proxy_pass http://fastapi_backend/openapi.json;
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            proxy_set_header Connection "";
        }

        # ====================================================================
        # ERROR PAGES
        # ====================================================================
        error_page 502 503 504 /50x.html;
        location = /50x.html {
            return 503 '{"error": "Service temporarily unavailable", "request_id": "$request_id"}';
            add_header Content-Type application/json;
        }

        error_page 429 /429.html;
        location = /429.html {
            return 429 '{"error": "Rate limit exceeded", "request_id": "$request_id"}';
            add_header Content-Type application/json;
        }
    }
}
